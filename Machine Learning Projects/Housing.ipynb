{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d68a8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import joblib\n",
    "#1)Get the Big Picture\n",
    "#We need to create a code to predict the Housing_Prices of a block in Califonia given certain metrics\n",
    "#median_house_value is capped to about $500,000, so is median_age, median_income is scaled down and capped at 15\n",
    "#This is a Regression Task, and not a Classification Task\n",
    "#2)Get The Data and Split it\n",
    "HOUSING_PATH='housing.csv'\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    return pd.read_csv(housing_path)\n",
    "housing=load_housing_data()\n",
    "#as income is an important metric, we need to ensure that the training data contains enough(not equal) represntation from all income categories or strata\n",
    "#but first, let's create a new attribute called income_category using the pd.cut() function\n",
    "housing['income_category']=pd.cut(housing['median_income'],bins=[0.,1.5,3.0,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "instance=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "#n_splits is the number of unique test-training sets we are creating from a dataset, and random state essentially ensures we get the same 'random' value across all runs \n",
    "#i.e, random to us but not the computer\n",
    "for train_index,test_index in instance.split(housing,housing['income_category']):\n",
    "    #instance.split.. is a generator class object that will create a fresh test-train index Series pair on the spot as the loop repeats according to it's n_splits value\n",
    "    #this way of spliting ensures that the percentage point of income category in the overall dataset and training set remain the same\n",
    "    train_set=housing.loc[train_index]\n",
    "    test_set=housing.loc[test_index]\n",
    "#now that we have split the dataset into train_set and test_set we can now get rid of income_category in both\n",
    "for set in [train_set,test_set]:\n",
    "    set.drop(['income_category'],inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "beed125a",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#3) Discover and Visualise the Data to get insights\n",
    "#First let's create a representation of blocks of Califonia by df.plot\n",
    "housing.plot(kind='scatter', x='latitude',y='longitude')\n",
    "#now,a better metric may be the density of blocks in a particular region, we may visualise the same by adjusting the transparency of the plot\n",
    "housing.plot(kind='scatter', x='latitude',y='longitude',alpha=0.1)\n",
    "#it is clear in the given plot that most blocks are concentrated in the southern region, that is where the ocean would be if it were a globe\n",
    "#now let's modify the graph once more to get insights about the population, income and prices\n",
    "housing.plot(kind='scatter', x='latitude',y='longitude',alpha=0.1,s=housing['population']/100,label=['population'],c=housing['median_house_value']\n",
    "            ,cmap=plt.get_cmap('jet'),colorbar=True)\n",
    "#From the given graph it is clear that the closer the block is to the ocean the prices sky rocket, or more appropriately, the more the population density(which tend to be clustered around the ocean) increases so does the housing prices\n",
    "#classic supply and demand, however ther are exceptions and other factors, note how in northern Califonia the housing prices are not that high despite it having a high population density"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6b062a4",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#So now we have to try and get a more ironclad correlation between the data and the labels, we can do this via simply the corr method\n",
    "corr_matrix=housing.corr()\n",
    "#corr_matrix is an x by x dataframe that gives the pearson's r(that is, the covariance of two variables\\the product of their standard deviations) which not only gives the direction of their variance(directly or inversely proportional)\n",
    "#but also how much, or how strongly they vary on a scale of -1 to 1\n",
    "print(corr_matrix['median_house_value'])\n",
    "#from the given Series, it is clear that house value is strongly and positvely related to median_income, and not so much on population overall"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19be5e52",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#Another way to check for correlations between attributes is to use Pandas' scatter_matrix fuction to plot every numerical attribute against every other attricute\n",
    "#however,as there are 11 numeric attributes, we get 11x11=121 unique dataframes, which is a bit too much\n",
    "#so we just focus on a few promising attributes\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes=['median_house_value','median_income','total_rooms','housing_median_age']\n",
    "scatter_matrix(housing[attributes],figsize=(12,8))\n",
    "#what we see here are mostly scatter plots, except for when it comes to plotting one attribute with itself, in which case the scatter_matrix uses a histogram(or else the main diagonal would have been full of straight lines)\n",
    "#The plot indeed reveals a few things, first the correlation from median_house_value with income is indeed very strong, second the income price cap is made apparant by the horizontal line around $500,000\n",
    "#third, this plot even reveals less obvious horizontallines around $450,000, $350,000 and possibly around $250,000 and perhaps a few more below that, you might want to remove these data quirks to prevent your algorithm from trying to reproduce these\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da53dd05",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#sometimes, it is convenient to experiment and combine various attributes in order to look for correlations in our data set\n",
    "housing['rooms_per_houshold']=housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room']=housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['person_per_room']=housing['population']/housing['total_rooms']\n",
    "corr_matrix=housing.corr()\n",
    "print(corr_matrix['median_house_value'].sort_values(ascending=False))\n",
    "#from the given Series, it is clear thar bedroom_per_room is strongly inversely related to the house value, turns out that houses with a lower bedroom to room ratio are more expensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b13dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#4)Prepare the data for Machine Learning Algorithms\n",
    "#First let's split the training set into predictors and labels\n",
    "housing=train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels=train_set['median_house_value'].copy()\n",
    "#We know that the total_bedrooms column has some empty values, sklearn provides a handy class to take care of missing values called SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy='median')\n",
    "#but first lets remove the ocean proximity from the dataset since the median can only be calculated for numerical values\n",
    "housing_num=housing.drop('ocean_proximity',axis=1)\n",
    "#imputer is an estimator(transfomer) object which estimates some values and transforms a given dataset\n",
    "imputer.fit(housing_num)\n",
    "#print(imputer.statistics_)\n",
    "X=imputer.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7cde444",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#Earlier we left out the categorial attribute ocean_proximity because it is a test attribute so we cannot compute it's median\n",
    "#Most Machine Learning algorithms prefer to work with numbers anyway so let's convert these categories from test to numbers.\n",
    "#For this, we can use the Scikit-Learn's OrdinalEncoder class(which is also an estimator)\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "housing_cat=housing[['ocean_proximity']]\n",
    "ordinal_encoder=OrdinalEncoder()\n",
    "housing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)\n",
    "print(housing_cat_encoded)\n",
    "print(ordinal_encoder.categories_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a190fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#however, this opens up another potential problem, by attatching linear numerical values to the attribute, the algorithm might even look into patterns that aren't even there intentionally\n",
    "# for example 1 is more closer to 2 than 4, which might just work on relative test values(such a Good, Great, Best etc)\n",
    "#however inland(1) is much more closer to nearbay(3) than it is to in ocean(0), which might cause a problem\n",
    "#One way to resolve this will be to make an additional column per category to bring it down to a simple yes(Hot), or No(Cold) question, with Yes being indicated by the number 1 and No being zero\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat=housing[['ocean_proximity']]\n",
    "cat_encoder=OneHotEncoder()\n",
    "housing_cat_1hot=cat_encoder.fit_transform(housing_cat)\n",
    "#print(type(housing_cat_1hot))\n",
    "#the resulting object is a sparse matrix as opposed to an ndarray, which is useful when you have columns in the thousands, to get an nd array we simply..\n",
    "#print(housing_cat_1hot.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e97602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#Custom Transformers\n",
    "#Although Scikit Learn gives us some useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes\n",
    "#We will need our new transformer to work with Scikit Learn functionalities(such as pipelines), luckily since Scikit Learn relys on Duck Typing and not inheritence, all we need to do is incoperate the three methods(fit, transform,fit_transform) in our new transoformer class\n",
    "#You can get fit_trandform for free if you include TranformerMixin as a Parent Class. Also, if you add add BaseEstimator as a Parent(and avoid *args, and **kwargs in your constructor) you will get two extra methods\n",
    "#(get_params() and setparams()) which will aid in hyperparameter tuning \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_index,bedrooms_index,population_index,households_index=3,4,5,6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "    def fit(self,X,y=None):\n",
    "        return self #nothing to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household=X[:,rooms_index]/X[:,households_index]\n",
    "        population_per_household=X[:,population_index]/X[:,households_index]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room=X[:,bedrooms_index]/X[:,rooms_index]\n",
    "            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]#catenating along 2nd axis\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65f54dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#esc+r to deactivate esc+y to activate\n",
    "#Perhaps one of the most useful techniques in Machine Learning is Feature Scaling, with a very few exceptions, most machine learning models, cannot work with vastly different magnitudes of attributes from each other\n",
    "#Two of the means Sklearn provides are MinMax scaling, and Standardisation\n",
    "#of the two MinMax scaler works best with most algortihms, as values are simply rescaled ranging from 0 to 1, we do this by subtracting the min value, dividing the max minus min, this method is also called normalisation\n",
    "#however, this method is also very susceptible to outliers, case in point, if the max value of a dataset is 100 while everything else falls below 15, then most of the dataset will also fall within 0-0.15, which is not ideal\n",
    "#for this purpose, sklearn also hands us the Standard Scaling tool, which is not affected by outliers much, but does not have any specific range for values, which make it difficult to work with in most algorithms\n",
    "#first, it substracts the mean value(so standardised values always have zero mean), and then it divides by the standard deviation so that the resulting distibution has unit variance, this method is called Standardisation\n",
    "#Transformation Pipelines\n",
    "#we often have to apply sequential transformations to our dataset, to streamline the process sklearn provides us with pipelines that could apply all transformations required in a sequential manner\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline=Pipeline([('imputer',SimpleImputer(strategy='median')),\n",
    "                     ('attribs-adder',CombinedAttributesAdder()),\n",
    "                     ('std_scaler',StandardScaler()),])\n",
    "#When used this way, all but the last estimator must be a transformer, the names of the estimator classes can be anything as long as they do not contain double underscores\n",
    "#when the pipeline's fit() is called, it will apply fit_transform() to all transformers except for the last one where it will simply stop by calling fit(), likewise transform() and fit_transform() both basically refer to the very last estimator\n",
    "#but, it is sometimes more advantageous to keep categorial(like KNearest) and numerical(like regression) in the same dataset, for this sklearn introduces a new class ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs=list(housing_num)\n",
    "cat_attribs=['ocean_proximity']\n",
    "\n",
    "full_pipeline=ColumnTransformer([('num', num_pipeline,num_attribs),\n",
    "                                ('cat',OneHotEncoder(),cat_attribs),])\n",
    "housing_final=full_pipeline.fit_transform(housing)\n",
    "#this applies both pipelines\\transformers seperately and then finally catenates the two along the second axis, in case of a mix between sparse and dense matrix, the ColumnTransformer shall only return a Sparse Matrix if the density(of the final sparse product) falls below a given threshold, which by default is set to 0.3\n",
    "#in this case the pipeline returns a dense matrix(i.e. 2D Arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdaec4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [ 85657.90192014 305492.60737488 152056.46122456 186095.70946094\n",
      " 244550.67966089]\n",
      "Labels:  [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]\n",
      "68627.87390018745\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#esc+r to deactivate esc+y to activate\n",
    "#5) Select and Train a Model\n",
    "#Sklearn comes with it's own inbuilt regression models, for starters let's train a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(housing_final,housing_labels)\n",
    "#..and done, let's test out this model with a portion of the data from the Train set\n",
    "some_data=housing.iloc[:5]\n",
    "some_labels=housing_labels.iloc[:5]\n",
    "some_data_prepped=full_pipeline.transform(some_data)\n",
    "print('Predictions: ',lin_reg.predict(some_data_prepped))\n",
    "print('Labels: ', list(some_labels))\n",
    "#it works!, though the predictions aren't entirely accurate, but to get a measure of it's accuracy, let's measure it using RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions=lin_reg.predict(housing_final)\n",
    "lin_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "#okay, it's better than nothing but it's still not good, this is a classic example of the model underfitting the data, to fix this there are mainly three steps we can take\n",
    "#Reduce the model's constrants, which is ruledd out since we haven't adjusted it in the slightest\n",
    "#feeding it better attributes, which is far too difficult\n",
    "#trying out a better model: Let's train a DecisionTreeRegressor, which is a powerful model capable of finding non linear relationships\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg=DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_final,housing_labels)\n",
    "housing_predictions=tree_reg.predict(housing_final)\n",
    "tree_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "tree_rmse=np.sqrt(tree_mse)\n",
    "print(tree_rmse)\n",
    "#while the rmse shows a value of zero, it's far more likely that the model has drastically overfit the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73c84583",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate\n",
    "#Better Evaluation using Cross Validation\n",
    "#if we test out the above model on the test set, we are unlikely to find anything close to the real labels, we could try messing with the hyperparameters to better fit the predictions closer to the labels, however as a general rule, we do not mess with the model after testing it on the test set to avoid sampling bias\n",
    "#a way to gauge a model's efficiency without overfitting the model on the test set is to simply train a model on a portion of the train set, and test it out on the other part, this method is called Validation\n",
    "#Cross Fold Validation splits the train set into parts called folds, and then trains the model to all except one of the folds, and tests it out on the remaining, likewise, all possible combinations on train sets and validation sets are tried out(in order to avoid sampling bias again) and their scores are returned, their average will indicate the performance measure of the model towards the entire problem\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(tree_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "tree_rmse_scores=np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard Deviation: \", scores.std())\n",
    "display_scores(tree_rmse_scores)\n",
    "#Now, the tree model doesn't look too good does it? another upside of this method, that it even gives us how accurate this estimate is(i.e. it's standard deviation), for example, if in half the iterations the estimate is really bad and in the other the estimate is really good, while the score would be in the middle of the errors, depending on the circumstances it can range from being really good, or really bad\n",
    "#to take the tree model's example, while the model's score is about 71551, depending on which train set and train set are used, the score can flip flop with a margin of error of 3139\n",
    "#Now, let's cross validate the linear regresion model just in case\n",
    "lin_scores=cross_val_score(lin_reg,housing_final,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores=np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n",
    "#yup, the tree overfits the data so much that it performs worse than the linear model\n",
    "#let's try one last model, called random forest regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "forest_reg.fit(housing_final,housing_labels)\n",
    "forest_scores=cross_val_score(forest_reg, housing_final, housing_labels, scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores=np.sqrt(-forest_scores)\n",
    "print(forest_rmse_scores)\n",
    "#there we go, forest regresor looks far more promissing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88243705",
   "metadata": {},
   "source": [
    "# esc+r to deactivate esc+y to activate\n",
    "#Now that we have a few promising models, now it is time to fine tune them, One way to do that is to change the hyper parameters manually\n",
    "#however that will require very tedious work. Instead let's use schikit Learn's GridSearchCV to search for us\n",
    "#all we need is to provide some hyperparameter values and it will search every possible combination using Cross Validation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[{'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},{'bootstrap':[False],'n_estimators':[3,10], 'max_features':[2,3,4]}]\n",
    "forest_reg=RandomForestRegressor()\n",
    "grid_search=GridSearchCV(forest_reg,param_grid,cv=5, scoring='neg_mean_squared_error',return_train_score=True)\n",
    "grid_search.fit(housing_final,housing_labels)\n",
    "print(grid_search.best_params_) #to get the best params\n",
    "my_model=grid_search.best_estimator_ #to automatically get the best estimator\n",
    "joblib.dump(my_model,\"my_model.pkl\")#to save model for later..later, use joblib.load(path) to load it\n",
    "#also, to get the evaluation results\n",
    "cvres=grid_search.cv_results_\n",
    "for mean_score,params in zip(cvres[\"mean_test_score\"],cvres['params']):\n",
    "    print(np.sqrt(-mean_score),params)\n",
    "#another approach is to use RandomisedSearchCV instead of GridSearchCV, which instead of trying all possible combinations, #randomly tries out a specified number of random values, This can be useful when we need to search a large sample space with #limited computing resources \n",
    "#You can even try combining differesnt models, for further efficiency, the group(or ensemble) usually performs better than the #individual"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7c3f3ab",
   "metadata": {},
   "source": [
    "#esc r to deactivate and esc y to activate\n",
    "#Now that we have our best models, we might get better insights on the problem by inspecting them. For example, The RandomForestRegressor can indicate the relative importance of each attribute for making accurate pedictions\n",
    "model=joblib.load('my_model.pkl')\n",
    "feature_importances=model.feature_importances_\n",
    "extra_attribs=['rooms_per_hhold','pop_per_hhold','bedrooms_per_room']\n",
    "cat_encoder=full_pipeline.named_transformers_['cat']#returns transformer or pipeline specified\n",
    "cat_attribs=list(cat_encoder.categories_[0])\n",
    "all_attribs=num_attribs+extra_attribs+cat_attribs\n",
    "print(sorted(zip(feature_importances,all_attribs)))\n",
    "#with this you can attempt to remove some of the less useful features(apperently only one ocean_proximity category is of any value, so you can try dropping the others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c504ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48109.13002677952\n",
      "[46111.59170126 50026.97167172]\n"
     ]
    }
   ],
   "source": [
    "##esc r to deactivate and esc y to activate\n",
    "#with this we have finally reached the last part of this project which is to test the model out on the test set\n",
    "final_model=joblib.load('my_model.pkl')\n",
    "X_test=test_set.drop('median_house_value',axis=1)\n",
    "Y_test=test_set['median_house_value'].copy()\n",
    "X_test_prepared=full_pipeline.transform(X_test)\n",
    "final_predictions=final_model.predict(X_test_prepared)\n",
    "final_mse=mean_squared_error(Y_test,final_predictions)\n",
    "final_rmse=np.sqrt(final_mse)\n",
    "print(final_rmse)#about 48,109\n",
    "#In some cases such point estimate of the generalisation error may not be enough to to convince you to launch, what if your new model is just 0.1% better than the model currently in production?\n",
    "#In such cases the precision of the error estimate becomes important, In which case it is often useful to find the confidence interval(usually 95%) of the estimate\n",
    "from scipy import stats\n",
    "confidence=0.95\n",
    "squared_errors=(final_predictions - Y_test)**2\n",
    "print(np.sqrt(stats.t.interval(confidence, len(squared_errors)-1,loc=squared_errors.mean(),scale=stats.sem(squared_errors))))\n",
    "#What this means is that if we tested this model with another tes set, there is a 95% chance that the rmse error would lie between the given range"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02a7ba97",
   "metadata": {},
   "source": [
    "#esc+r to deactivate esc+y to activate \n",
    "#2 Alternative) to split data using identifier\n",
    "from zlib import crc32\n",
    "def test_set_check(identifier,test_ratio):\n",
    "    return crc32(np.int32(identifier)) & 0xffffffff < test_ratio*2**32#crc32 'uniformly' distributes unique values across allowed 32bit constraint \n",
    "#and 2 raised to 32 is the maximum value a 32 bit integer can take, &0xffffffff converts signed values to unsigned values(only for python2)\n",
    "\n",
    "def  split_dataset_by_id(data,test_ratio,id_column):\n",
    "    ids=data[id_column] # returns index Pandas Series\n",
    "    test_check=ids.apply(lambda id : test_set_check(id,test_ratio))#this will return a True-False 1-D DF\n",
    "    return data[test_check], data[~test_check]#works because test_check is a superimposable Series and ~argument inverts the booleen values in a Series\n",
    "housing_with_id=housing.reset_index()\n",
    "test_set,training_set=split_dataset_by_id(housing_with_id,0.2,'index')\n",
    "print(test_set,training_set)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
